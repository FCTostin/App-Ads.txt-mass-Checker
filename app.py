import streamlit as st
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="App-ads.txt Checker", 
    layout="wide", 
    page_icon="üõ°Ô∏è"
)

# –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
st.markdown("""
    <style>
    .stApp { background-color: #0e1117; color: #fafafa; }
    .stTextArea textarea { background-color: #262730; color: #ffffff; border: 1px solid #444; }
    div[data-testid="stDataFrame"] { background-color: #262730; }
    </style>
""", unsafe_allow_html=True)

st.title("üõ°Ô∏è App-ads.txt Checker")
st.markdown("Verifies file availability and counts **valid IAB ad records**.")

LIVE_UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

def get_session():
    session = requests.Session()
    session.headers.update({
        'User-Agent': LIVE_UA,
        'Accept': 'text/plain,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    })
    return session

def clean_domain(url_input):
    url_input = url_input.strip().replace('"', '').replace("'", "")
    if not url_input.startswith(("http://", "https://")):
        url_input = "http://" + url_input
    try:
        parsed = urlparse(url_input)
        domain = parsed.netloc if parsed.netloc else parsed.path.split('/')[0]
        return domain.lower().strip()
    except:
        return url_input

def count_valid_lines(content):
    """
    –ü–∞—Ä—Å–∏—Ç –∫–æ–Ω—Ç–µ–Ω—Ç app-ads.txt —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º IAB.
    –£—Å—Ç–æ–π—á–∏–≤ –∫ BOM, —Ä–∞–∑–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º –∏ –Ω–µ–≤–∏–¥–∏–º—ã–º —Å–∏–º–≤–æ–ª–∞–º.
    """
    valid_count = 0
    # –£–¥–∞–ª—è–µ–º Byte Order Mark (BOM), –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    content = content.replace('\ufeff', '')
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    lines = content.replace('\r\n', '\n').replace('\r', '\n').splitlines()
    
    for line in lines:
        # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–≤—Å–µ, —á—Ç–æ –ø–æ—Å–ª–µ #)
        clean_line = line.split('#')[0].strip()
        
        if not clean_line:
            continue
            
        # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ –∑–∞–ø—è—Ç—ã–º
        parts = [p.strip() for p in clean_line.split(',')]
        
        # –ó–∞–ø–∏—Å—å –≤–∞–ª–∏–¥–Ω–∞, –µ—Å–ª–∏ –≤ –Ω–µ–π –µ—Å—Ç—å: Domain, Account ID, Type (DIRECT/RESELLER)
        if len(parts) >= 3:
            # –û—á–∏—â–∞–µ–º –ø–æ–ª–µ —Ç–∏–ø–∞ –∑–∞–ø–∏—Å–∏ –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–µ–≤–∏–¥–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã A-Z
            relationship = re.sub(r'[^A-Z]', '', parts[2].upper())
            
            if relationship in ["DIRECT", "RESELLER"]:
                valid_count += 1
                
    return valid_count

def check_domain_smart(domain):
    session = get_session()
    # –ü–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ app-ads.txt –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –∫–æ—Ä–Ω–µ –∏–ª–∏ –≤ –ø–æ–¥–ø–∞–ø–∫–µ (–Ω–æ —á–∞—â–µ –≤ –∫–æ—Ä–Ω–µ)
    urls_to_try = [f"https://{domain}/app-ads.txt", f"http://{domain}/app-ads.txt"]
    
    for url in urls_to_try:
        try:
            response = session.get(url, timeout=12, allow_redirects=True, verify=True)
            
            # –ï—Å–ª–∏ SSL –æ—à–∏–±–∫–∞ ‚Äî –ø—Ä–æ–±—É–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞ (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–∫–æ–Ω–æ–º—è—Ç –Ω–∞ SSL)
            if response.status_code == 200:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ "–∫—Ä–∞–∫–æ–∑—è–±—Ä"
                response.encoding = response.apparent_encoding
                content = response.text
                
                # –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª HTML –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (—á–∞—Å—Ç–∞—è –æ—à–∏–±–∫–∞ 404-—Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤)
                if "<html" in content.lower()[:200] or "<!doctype" in content.lower()[:200]:
                    continue 
                
                valid_lines = count_valid_lines(content)
                return url, "Valid", valid_lines
                
        except requests.exceptions.SSLError:
            try:
                # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ SSL
                response = session.get(url, timeout=10, allow_redirects=True, verify=False)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    content = response.text
                    if "<html" not in content.lower()[:200]:
                        valid_lines = count_valid_lines(content)
                        return url, "Valid", valid_lines
            except:
                pass
        except Exception:
            pass
            
    return f"https://{domain}/app-ads.txt", "Error / Not Found", 0

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
input_text = st.text_area("Insert domain list (1 per line)", height=300, placeholder="example.com\nhyperhippo.com")

if st.button("Run Check"):
    if not input_text.strip():
        st.warning("The list is empty.")
    else:
        raw_lines = [line.strip() for line in input_text.splitlines() if line.strip()]
        
        tasks = []
        for idx, line in enumerate(raw_lines):
            domain = clean_domain(line)
            tasks.append((idx, domain))
            
        st.info(f"Analyzing {len(tasks)} domains...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        unsorted_results = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (20 –ø–æ—Ç–æ–∫–æ–≤)
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_idx = {
                executor.submit(check_domain_smart, domain): idx 
                for idx, domain in tasks
            }
            
            completed = 0
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    url, status, lines = future.result()
                except Exception as e:
                    orig_domain = tasks[idx][1]
                    url = f"https://{orig_domain}/app-ads.txt"
                    status = "Error"
                    lines = 0
                
                unsorted_results.append({
                    "Original_Index": idx,
                    "App-ads Link": url,
                    "Status": status,
                    "Valid Lines": lines
                })
                
                completed += 1
                progress_bar.progress(completed / len(tasks))
                status_text.text(f"Processed: {completed}/{len(tasks)}")
        
        progress_bar.empty()
        status_text.empty()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø–æ—Ä—è–¥–∫–µ –≤–≤–æ–¥–∞
        df = pd.DataFrame(unsorted_results)
        df = df.sort_values(by="Original_Index").drop(columns=["Original_Index"])
        
        st.subheader("Results")
        
        def highlight_row(val):
            if val == "Valid":
                return 'color: #4CAF50; font-weight: bold'
            return 'color: #FF5252; font-weight: bold'
            
        st.dataframe(
            df.style.map(highlight_row, subset=['Status']),
            use_container_width=True,
            hide_index=True,
            column_config={
                "App-ads Link": st.column_config.LinkColumn("App-ads Link"),
                "Valid Lines": st.column_config.NumberColumn("Valid Lines (IAB)", help="Number of records matching IAB standards")
            }
        )
        
        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button("üíæ Download Report (CSV)", csv_data, "app_ads_check_results.csv", "text/csv")
