import streamlit as st
import pandas as pd
from curl_cffi import requests as cffi_requests # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import re

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
st.set_page_config(
    page_title="App-ads.txt Checker", 
    layout="wide", 
    page_icon="üõ°Ô∏è"
)

st.markdown("""
    <style>
    .stApp { background-color: #0e1117; color: #fafafa; }
    .stTextArea textarea { background-color: #262730; color: #ffffff; border: 1px solid #444; }
    div[data-testid="stDataFrame"] { background-color: #262730; }
    </style>
""", unsafe_allow_html=True)

st.title("üõ°Ô∏è App-ads.txt Checker")
st.markdown("Verifies **app-ads.txt** availability using **Browser Impersonation** (similar to Chrome Extension).")

# --- –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–º–µ–Ω–∞ ---
def clean_domain(url_input):
    url_input = url_input.strip().replace('"', '').replace("'", "")
    if not url_input.startswith(("http://", "https://")):
        url_input = "http://" + url_input   
    try:
        parsed = urlparse(url_input)
        domain = parsed.netloc if parsed.netloc else parsed.path.split('/')[0]
        # –£–±–∏—Ä–∞–µ–º www, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç —Å–∞–º –ø–µ—Ä–µ–±–∏—Ä–∞–ª –≤–∞—Ä–∏–∞–Ω—Ç—ã
        return domain.lower().replace("www.", "").strip()
    except:
        return url_input

# --- –ü–∞—Ä—Å–∏–Ω–≥ (–¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ IAB) ---
def count_valid_lines(content):
    valid_count = 0
    # –£–¥–∞–ª—è–µ–º BOM –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    content = content.replace('\ufeff', '')
    lines = content.replace('\r\n', '\n').replace('\r', '\n').splitlines()
    
    for line in lines:
        clean_line = line.split('#')[0].strip()
        if not clean_line:
            continue
            
        parts = [p.strip() for p in clean_line.split(',')]
        
        # IAB —Å—Ç–∞–Ω–¥–∞—Ä—Ç: Domain, Account ID, Type
        if len(parts) >= 3:
            # –û—á–∏—Å—Ç–∫–∞ —Ç–∏–ø–∞ –æ—Ç –º—É—Å–æ—Ä–∞
            relationship = re.sub(r'[^A-Z]', '', parts[2].upper())
            if relationship in ["DIRECT", "RESELLER"]:
                valid_count += 1
                
    return valid_count

# --- –ó–∞–ø—Ä–æ—Å —Å –∏–º–∏—Ç–∞—Ü–∏–µ–π –±—Ä–∞—É–∑–µ—Ä–∞ ---
def fetch_url_impersonate(url):
    try:
        # impersonate="chrome120" -> –°–µ—Ä–≤–µ—Ä –¥—É–º–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ –Ω–∞—Å—Ç–æ—è—â–∏–π –±—Ä–∞—É–∑–µ—Ä
        response = cffi_requests.get(
            url, 
            impersonate="chrome120", 
            timeout=15,
            allow_redirects=True
        )
        
        if response.status_code == 200:
            content = response.text
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ HTML-–∑–∞–≥–ª—É—à–∫–∏ (–∏–Ω–æ–≥–¥–∞ —Å–µ—Ä–≤–µ—Ä –æ—Ç–¥–∞–µ—Ç 200, –Ω–æ —ç—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—à–∏–±–∫–∏)
            if "<html" in content.lower()[:300] or "<!doctype" in content.lower()[:300]:
                return None
            return content
    except Exception:
        return None
    return None

# --- –£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (—Ç–æ–ª—å–∫–æ app-ads.txt) ---
def check_domain_smart(domain):
    # –ü—Ä–æ–±—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å www –∏ –±–µ–∑. 
    # HyperHippo —Ç—Ä–µ–±—É–µ—Ç www, –¥—Ä—É–≥–∏–µ —Å–∞–π—Ç—ã –Ω–∞–æ–±–æ—Ä–æ—Ç. –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ.
    urls_to_try = [
        f"https://www.{domain}/app-ads.txt", # –ß–∞—Å—Ç—ã–π –∫–µ–π—Å –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —Å—Ç—É–¥–∏–π
        f"https://{domain}/app-ads.txt",
        f"http://www.{domain}/app-ads.txt",
        f"http://{domain}/app-ads.txt"
    ]
    
    for url in urls_to_try:
        content = fetch_url_impersonate(url)
        
        if content:
            valid_lines = count_valid_lines(content)
            
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–∞–π–¥–µ–Ω –∏ –≤–∞–ª–∏–¥–µ–Ω (–∏–ª–∏ –ø—É—Å—Ç, –Ω–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏)
            if valid_lines >= 0:
                # –ï—Å–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç, –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ "Empty File", –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî "Valid"
                status = "Valid" if valid_lines > 0 else "File Empty (0 lines)"
                return url, status, valid_lines
            
    return f"https://{domain}/app-ads.txt", "Error / Not Found", 0

# --- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
input_text = st.text_area(
    "Insert domain list (1 per line)", 
    height=300, 
    placeholder="hyperhippo.com\ngoogle.com"
)

if st.button("Run Check"):
    if not input_text.strip():
        st.warning("The list is empty.")
    else:
        raw_lines = [line.strip() for line in input_text.splitlines() if line.strip()]
        
        tasks = []
        for idx, line in enumerate(raw_lines):
            domain = clean_domain(line)
            tasks.append((idx, domain))
            
        st.info(f"Analyzing {len(tasks)} domains...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        unsorted_results = []
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_idx = {
                executor.submit(check_domain_smart, domain): idx 
                for idx, domain in tasks
            }
            
            completed = 0
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    url, status, lines = future.result()
                except Exception:
                    orig_domain = tasks[idx][1]
                    url = f"https://{orig_domain}/app-ads.txt"
                    status = "Error"
                    lines = 0
                
                unsorted_results.append({
                    "Original_Index": idx,
                    "App-ads Link": url,
                    "Status": status,
                    "Valid Lines": lines
                })
                
                completed += 1
                progress_bar.progress(completed / len(tasks))
                status_text.text(f"Processed: {completed}/{len(tasks)}")
        
        progress_bar.empty()
        status_text.empty()
        
        df = pd.DataFrame(unsorted_results)
        df = df.sort_values(by="Original_Index").drop(columns=["Original_Index"])
        
        st.subheader("Results")
        
        def highlight_row(val):
            if "Valid" in str(val):
                return 'color: #4CAF50; font-weight: bold'
            return 'color: #FF5252; font-weight: bold'
            
        st.dataframe(
            df.style.map(highlight_row, subset=['Status']),
            use_container_width=True,
            hide_index=True,
            column_config={
                "App-ads Link": st.column_config.LinkColumn("App-ads Link"),
                "Valid Lines": st.column_config.NumberColumn("Valid Lines", help="Valid IAB records found")
            }
        )
        
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button("üíæ Download Report (CSV)", csv_data, "app_ads_check_results.csv", "text/csv")
